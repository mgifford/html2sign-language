# SignAvatars Integration Summary

## ‚úÖ What's Been Set Up

### 1. Files Discovered & Extracted
- **SMPL-X Body Models**: `signavatars-data/models/smplx/` (extracted from `models_smplx_v1_1.zip`)
  - SMPLX_NEUTRAL.npz (108 MB)
  - SMPLX_MALE.npz
  - SMPLX_FEMALE.npz
  
- **Animation Data**: `signavatars-data/asl-word-level/` (extracted from `wlasl_pkls_cropFalse_defult_shape.zip`)
  - Hundreds of .pkl files (00295.pkl, 00333.pkl, etc.)
  - Each file contains 20-30 frames of SMPL-X parameters for one ASL word
  - Format: PyTorch pickle with keys: 'smplx', 'left_valid', 'right_valid', '2d', etc.

### 2. Python Environment
```bash
Location: .venv/
Python: 3.13.2
Packages installed:
  - torch (for loading .pkl files)
  - smplx (body model)
  - trimesh (mesh processing)
  - pyrender
  - numpy, scipy, pillow
```

### 3. Conversion Script Created
**File**: `convert_pkl_to_glb.py`

**Key Features**:
- Custom `CPU_Unpickler` to handle CUDA tensor files on CPU-only machines
- SignAvatars format parser (182 parameters per frame)
- Parameter layout:
  - global_orient: 3
  - body_pose: 63
  - left_hand_pose: 12 (padded to 45 for SMPL-X)
  - right_hand_pose: 12 (padded to 45)
  - jaw_pose: 3
  - leye_pose: 3
  - reye_pose: 3
  - expression: 10
  - betas: 10
  - transl: 3

**Test Result**:
```bash
‚úÖ Successfully converted: signavatars-data/asl-word-level/00295.pkl
‚úÖ Output: animations/test-00295.glb
‚úÖ 21 frames processed, 10,475 vertices, 20,908 faces
```

## üîÑ Next Steps

### Batch Conversion
The workflow script needs updating to use the new `.pkl` format:

```bash
# Find all .pkl files
find signavatars-data/asl-word-level -name "*.pkl" | while read file; do
  basename=$(basename "$file" .pkl)
  .venv/bin/python convert_pkl_to_glb.py \
    --input "$file" \
    --output "animations/asl-$basename.glb" \
    --word "$basename"
done
```

### Microsoft Accessible-Connect Analysis

**Repository**: https://github.com/microsoft/Accessible-Connect

**What it does**: Video conferencing platform with AI gesture recognition for accessibility
- MediaPipe Holistic for hand tracking
- Real-time gesture detection (hand raise, thumbs up, clap, OK)
- Live captions/transcriptions
- Participant types (Deaf/Hearing/Interpreter)

**NOT useful for this project**:
- ‚ùå No 3D avatar rendering
- ‚ùå No sign language synthesis/generation
- ‚ùå Video-based input recognition, not avatar animation output

**Potentially useful**:
- ‚úÖ MediaPipe integration patterns (if we want gesture input later)
- ‚úÖ Accessibility UI/UX patterns (captions, transcriptions)
- ‚úÖ Participant type management concepts

**Recommendation**: Not relevant for current avatar animation pipeline. Focus on SignAvatars dataset conversion instead.

## üéØ Immediate Action Items

1. **Create Word-to-File Mapping**
   - The .pkl files are numbered (00295.pkl, 00333.pkl)
   - Need to map these to actual ASL words (e.g., 00295 ‚Üí "hello", 00333 ‚Üí "thank you")
   - Check if SignAvatars provides a glossary file (likely in homogenus_v1_0.zip)

2. **Batch Convert Top 100 Words**
   ```bash
   # Convert first 100 .pkl files for testing
   ls signavatars-data/asl-word-level/*.pkl | head -100 | while read file; do
     basename=$(basename "$file" .pkl)
     .venv/bin/python convert_pkl_to_glb.py \
       --input "$file" \
       --output "animations/asl-$basename.glb" \
       --word "$basename"
   done
   ```

3. **Implement Multi-Frame Animation Export**
   - Current: Only exports frame 0 as static mesh
   - TODO: Add GLB animation tracks for all 21 frames
   - Reference: https://github.com/KhronosGroup/glTF-Tutorials/blob/master/gltfTutorial/gltfTutorial_007_Animations.md

4. **Update signs.json**
   - Auto-populate with converted animations
   - Include biomechanical validation flag (all SignAvatars are validated)
   - Add HamNoSys notation if available

5. **Test in Browser**
   - Load converted GLB files in index.html
   - Verify placeholder detection is removed
   - Test compound word breakdown with real signs

## üìä Progress Status

| Task | Status | Notes |
|------|--------|-------|
| Download SMPL-X models | ‚úÖ | All gender variants available |
| Extract SignAvatars data | ‚úÖ | Hundreds of .pkl files ready |
| Python environment setup | ‚úÖ | All dependencies installed |
| .pkl ‚Üí GLB conversion script | ‚úÖ | Working with test file |
| Multi-frame animation export | ‚è≥ | TODO: Implement GLB animation |
| Word-to-file mapping | ‚è≥ | Need to extract from glossary |
| Batch conversion | ‚è≥ | Script ready, awaiting execution |
| Browser integration testing | ‚è≥ | Awaiting converted files |

## üîß Technical Details

### SignAvatars Format Quirks
1. **Hand Pose Dimensions**: SignAvatars uses 12-param simplified hand model (4 fingers √ó 3 rotations). SMPL-X expects 45 params (15 joints √ó 3). Solution: Pad with zeros.

2. **PyTorch CUDA Tensors**: Files were saved with CUDA device references. Solution: Custom unpickler with recursive CPU mapping.

3. **PCA Hand Pose**: SMPL-X defaults to 6-param PCA hand pose. Solution: Disable PCA with `use_pca=False, flat_hand_mean=True`.

### File Sizes
- Each .pkl: ~100-300 KB
- Each converted .glb: ~1-2 MB (single frame)
- Total dataset: ~100+ animations available

### Performance
- Conversion speed: ~21 frames in ~5 seconds
- Static GitHub Pages compatible: ‚úÖ
- No server runtime needed: ‚úÖ
